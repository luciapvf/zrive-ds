{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T17:58:04.363802Z",
     "start_time": "2025-11-06T17:58:04.302362Z"
    }
   },
   "cell_type": "code",
   "source": "# DOCUEMNTO DE DESCRIPCIÓN TÉCNICA - TDD",
   "id": "c8aed8ed28356166",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T17:58:55.448835Z",
     "start_time": "2025-11-06T17:58:41.882250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from plotnine import ggplot, aes, geom_point, geom_smooth, labs, theme_bw\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "514b763780112a25",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T18:26:53.162002Z",
     "start_time": "2025-11-06T18:26:53.128545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "CONTEXTO:\n",
    "Enviar notificaciones push a clientes para animarlos a comprar un producto que ya han seleccionado previamente.\n",
    "\n",
    "OBJETIVO:\n",
    "Desarrollar un modelo de aprendizaje automático que dado un usuarioo y un producto prediga si el usuario\n",
    "compraría el produco si estuviera comprando con nosotros en ese momento.\n",
    "\n",
    "DATOS:\n",
    "Usar la base de datos feature_frame.csv filtrando sólo los pedidos de al menos 5 productos. En este caso,\n",
    "uso directamente feature_frame_filtered.csv que filtra dichos pedidos (usado en el módulo anterior).\n",
    "\"\"\""
   ],
   "id": "e8adc9d0f2970024",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCONTEXTO:\\nEnviar notificaciones push a clientes para animarlos a comprar un producto que ya han seleccionado previamente.\\n\\nOBJETIVO:\\nDesarrollar un modelo de aprendizaje automático que dado un usuarioo y un producto prediga si el usuario\\ncompraría el produco si estuviera comprando con nosotros en ese momento.\\n\\nDATOS:\\nUsar la base de datos feature_frame.csv filtrando sólo los pedidos de al menos 5 productos. En este caso,\\nuso directamente feature_frame_filtered.csv que filtra dichos pedidos (usado en el módulo anterior).\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T18:26:53.676543Z",
     "start_time": "2025-11-06T18:26:53.646321Z"
    }
   },
   "cell_type": "code",
   "source": "# HITO 1: FASE DE EXPLORACIÓN (Construcción del modelo predictivo)",
   "id": "7fca24d794d3b1dd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T18:26:59.410634Z",
     "start_time": "2025-11-06T18:26:54.312184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Se cargan los datos\n",
    "BASE_DIR = os.getcwd()  # obtiene el directorio actual\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"feature_frame_filtered.csv\")\n",
    "\n",
    "print(\"Cargando datos desde:\", DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Datos cargados correctamente:\", df.shape)"
   ],
   "id": "759ea173cbc87dc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde: C:\\Users\\Lucia\\PycharmProjects\\zrive-ds\\src\\module_4\\.ipynb_checkpoints\\data\\feature_frame_filtered.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Lucia\\\\PycharmProjects\\\\zrive-ds\\\\src\\\\module_4\\\\.ipynb_checkpoints\\\\data\\\\feature_frame_filtered.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m DATA_PATH = os.path.join(BASE_DIR, \u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfeature_frame_filtered.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mCargando datos desde:\u001B[39m\u001B[33m\"\u001B[39m, DATA_PATH)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mDatos cargados correctamente:\u001B[39m\u001B[33m\"\u001B[39m, df.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Lucia\\\\PycharmProjects\\\\zrive-ds\\\\src\\\\module_4\\\\.ipynb_checkpoints\\\\data\\\\feature_frame_filtered.csv'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head # para ver las primeras filas",
   "id": "2c002e98f977c746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info() # para ver tipo de datos, si hay nulos",
   "id": "b9354feb7b24dcab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clasificación de columnas\n",
    "info_col = [\"variant_id\",\"order_id\",\"user_id\",\"created_at\",\"order_date\"] # id y fechas (son distintos de cada pedido)\n",
    "label_col = \"outcome\" # variable objetivo (y)\n",
    "features_col = [ col for col in df.columns if col not in info_col + [label_col]] # resto de columnas\n",
    "\n",
    "categorical_col = [\"predict_type\",\"vendor\"] # columnas categóricas (de las features)\n",
    "binary_col = [\"ordered_before\", \"abandoned_before\", \"active_snoozed\", \"set_as_regular\"] # columnas binarias (de las features)\n",
    "numerical_col = [col for col in features_col if col not in categorical_col + binary_col] # resto (de las features)"
   ],
   "id": "a3213bb0a94f63a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 2. Proceso de entrenamiento, validación y test",
   "id": "1327551b8da021f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOTA: Se tiene que hacer un SPLIT TEMPORAL, para evitar INFORMATION LEAKAGE. Es decir, respeto el orden temportal de los \n",
    "# datos. Quiero que el proceso de entretanmiento, validación sean lo más cercano posible al proceso de producción."
   ],
   "id": "f3c3bd8973d8a7f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# En la gráfica de los perdidos por días, los dividimos en 3 columnas y cada parte se encarga de hacer train, val y test.\n",
    "daily_orders = df.groupby(\"order_date\").order_id.nunique() # pedidos diarios\n",
    "daily_orders.head() # primeras filas\n",
    "\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"]) # eje x, convertir a datetime si no lo es\n",
    "daily_orders = df.groupby(\"order_date\").order_id.nunique() # agrupo por fecha\n",
    "\n",
    "plt.plot(daily_orders, label=\"daily_orders\") # presentación\n",
    "plt.title(\"Pedidos Diarios\")"
   ],
   "id": "f072ae545780ffcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# De este modo, una orden (pedido) o está en train, o en validación o en test. ",
   "id": "d32893a571315c78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Entreno con el 70% de los datos y valido con el 90%. Para ello, creo una función que me defina el nº de pedidos acumulativos.\n",
    "orders_acum = daily_orders.cumsum() / daily_orders.sum()\n",
    "\n",
    "plt.plot(orders_acum, label=\"orders_acum\") # represento (tiene que dar 1 la suma obv)\n",
    "plt.title(\"Suma acumulativa de pedidos\")"
   ],
   "id": "c43cd26d45322212"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Realizo la división (cut off).\n",
    "train_val_cutoff = orders_acum [orders_acum <= 0.7].idxmax()\n",
    "val_test_cutoff = orders_acum [orders_acum <= 0.9].idxmax()\n",
    "\n",
    "print(\"Entrenamiendo desde:\", orders_acum.index.min())\n",
    "print(\"Entranmiento hasta:\", train_val_cutoff)\n",
    "print(\"Validación hasta:\", val_test_cutoff)\n",
    "print(\"Test hasta:\", orders_acum.index.max())"
   ],
   "id": "9d5e6981c2bc0953"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_df = df[df.order_date <= train_val_cutoff]\n",
    "val_df = df[(df.order_date > train_val_cutoff) & (df.order_date <= val_test_cutoff)]\n",
    "test_df = df[df.order_date > val_test_cutoff]"
   ],
   "id": "38f7580961456f28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BASELINE: Siempre hay que hacer una baseline, es decir, un modelo sencillo que nos de métricas que queremos mejorar para producción.\n",
    "# En este caso, como baseline uso la variable product_popularity, ¿cómo de popular es un producto?. La probabilidad de vender un \n",
    "# producto depende de su popularidad."
   ],
   "id": "ad411d6284c081d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# El modelo (baseline) predice la prob. de que un usuario compre un producto si recibe una notificación push. Sin embargo:\n",
    "# Si envío una notificación y el usuario no compra, le molesta → coste negativo (falso positivo).\n",
    "# Si no envío y el usuario habría comprado, pierdo una venta → coste de oportunidad (falso negativo).\n",
    "# Por tanto, no debo enviar notificaciones a todos los usuarios con prob > 0.5, sino solo cuando la ganancia esperada sea positiva."
   ],
   "id": "5ef1271d73870c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def plot_metrics(model_name, y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Dibuja curvas Precision–Recall y ROC para un modelo binario.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Nombre del modelo para la leyenda.\n",
    "    y_pred : array-like\n",
    "        Probabilidades predichas (no etiquetas).\n",
    "    y_test : array-like\n",
    "        Etiquetas reales (0/1).\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    fig, ax : matplotlib.figure.Figure, matplotlib.axes._subplots.AxesSubplot\n",
    "        Figura y ejes de los subplots.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================\n",
    "    # Curva Precision–Recall\n",
    "    # ==========================\n",
    "    recall_, precision_, _ = precision_recall_curve(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_pred)  # PR-AUC estable\n",
    "\n",
    "    # ==========================\n",
    "    # Curva ROC\n",
    "    # ==========================\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # ==========================\n",
    "    # Crear figura con dos subplots\n",
    "    # ==========================\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- Precision–Recall ---\n",
    "    ax[0].plot(recall_, precision_, color='royalblue',\n",
    "               label=f\"{model_name}\\nAP={avg_precision:.3f}\")\n",
    "    ax[0].set_xlabel(\"Recall\")\n",
    "    ax[0].set_ylabel(\"Precision\")\n",
    "    ax[0].set_title(\"Curva Precision–Recall\")\n",
    "    ax[0].set_xlim(0, max(recall_)*1.05)   # zoom dinámico\n",
    "    ax[0].set_ylim(0, max(precision_)*1.05)\n",
    "    ax[0].legend(loc=\"upper right\", fontsize=9)\n",
    "    ax[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # --- ROC ---\n",
    "    ax[1].plot(fpr, tpr, color='darkorange',\n",
    "               label=f\"{model_name}\\nAUC={roc_auc:.3f}\")\n",
    "    ax[1].plot([0,1],[0,1],'--',color='gray', alpha=0.6)\n",
    "    ax[1].set_xlabel(\"False Positive Rate\")\n",
    "    ax[1].set_ylabel(\"True Positive Rate\")\n",
    "    ax[1].set_title(\"Curva ROC\")\n",
    "    ax[1].legend(loc=\"lower right\", fontsize=9)\n",
    "    ax[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ],
   "id": "9bce549c09c60057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plot_metrics(\n",
    "    \"Popularity baseline\",\n",
    "    y_pred=val_df[\"global_popularity\"],\n",
    "    y_test=val_df[label_col]\n",
    ")\n",
    "\n",
    "plt.show()  # ✅ Obligatorio en PyCharm\n"
   ],
   "id": "f9331cbb5a7fd6b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ahora realizamos un entrenamiento con 2 modelos lineales (L1, L2) y diferentes hiperparámetros.\n",
    "# Preprocesamiento + pipeline + entrenamiento + evaluación."
   ],
   "id": "650890f0f845e1f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocesamiento\n",
    "categorical_col = [\"product_type\",\"vendor\"]\n",
    "binary_col = [\"ordered_before\", \"abandoned_before\", \"active_snoozed\", \"set_as_regular\"]\n",
    "numerical_col = [col for col in features_col if col not in categorical_col + binary_col]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_col), # standardscaler -> media 0 sd 1\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_col)\n",
    "    ],\n",
    "    remainder='passthrough'  # columnas binarias se mantienen tal cual\n",
    ")\n",
    "\n",
    "# Conjuntos\n",
    "X_train = train_df[features_col]\n",
    "y_train = train_df[label_col]\n",
    "X_val = val_df[features_col]\n",
    "y_val = val_df[label_col]"
   ],
   "id": "bfa04158b230cff4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Entrenamiento y gráficas\n",
    "predictions = {}\n",
    "estimators = {}\n",
    "scores = {}\n",
    "\n",
    "# Valores de C a explorar\n",
    "C_values = [0.01, 0.1, 1]\n",
    "\n",
    "for penalty in [\"l1\", \"l2\"]:\n",
    "    print(f\"\\n=== Entrenando modelo con penalización {penalty} ===\")\n",
    "\n",
    "    # Definir el modelo base según la penalización\n",
    "    solver = \"liblinear\" if penalty == \"l1\" else \"lbfgs\"\n",
    "    log_reg = LogisticRegression(penalty=penalty, solver=solver, max_iter=1000)\n",
    "\n",
    "    # GridSearchCV para encontrar el mejor C\n",
    "    grid = GridSearchCV(log_reg, param_grid={\"C\": C_values}, scoring=\"average_precision\", cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Mejor modelo y C encontrado\n",
    "    best_C = grid.best_params_[\"C\"]\n",
    "    best_score = grid.best_score_\n",
    "    print(f\"Mejor C para {penalty}: {best_C}, score medio CV={best_score:.4f}\")\n",
    "\n",
    "    # Predicciones sobre el conjunto de validación\n",
    "    y_pred_val = grid.predict_proba(X_val)[:, 1]\n",
    "    ap_val = average_precision_score(y_val, y_pred_val)\n",
    "    print(f\"Average Precision en validación: {ap_val:.4f}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    predictions[penalty] = y_pred_val\n",
    "    estimators[penalty] = grid.best_estimator_\n",
    "    scores[penalty] = ap_val\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# GRÁFICAS ROC Y PRECISIÓN–RECALL\n",
    "# =====================================================\n",
    "penalties = list(predictions.keys())\n",
    "\n",
    "# ---------------------\n",
    "# CURVA ROC\n",
    "# ---------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for penalty in penalties:\n",
    "    y_pred = predictions[penalty]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{penalty.upper()} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel(\"Tasa de Falsos Positivos (1 - Especificidad)\")\n",
    "plt.ylabel(\"Tasa de Verdaderos Positivos (Sensibilidad)\")\n",
    "plt.title(\"Curvas ROC para diferentes penalizaciones\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# CURVA PRECISIÓN–RECALL\n",
    "# ---------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for penalty in penalties:\n",
    "    y_pred = predictions[penalty]\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred)\n",
    "    ap = average_precision_score(y_val, y_pred)\n",
    "    plt.plot(recall, precision, lw=2, label=f\"{penalty.upper()} (AP = {ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall (Sensibilidad)\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.title(\"Curvas Precisión–Recall para diferentes penalizaciones\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4cd8cae69f3ea8c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# NO ME LAS DIBUJA. ME FALTA EL DIBUJO, DECIR QUÉ FUNCIONÓ Y QUE NO Y POR QUÉ",
   "id": "94d8dfe8f34edb41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MODELOS NO LINEALES: Random Forests y Boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "# n_estimators = nº de árboles\n",
    "# max_depth = profundidad máxima, controla sobreajuste → regularización\n",
    "# min_samples_leaf = nº mínimo de muestras por hoja, también regulariza\n",
    "# learning_rate = una de las regularizaciones más importantes\n",
    "# subsample = proporción de muestras usadas en cada árbol, regulariza al añadir aleatoriedad\n",
    "\n",
    "# Configuración general para guardar valores\n",
    "predictions = {}\n",
    "scores = {}"
   ],
   "id": "2f06d7cfd1c843f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Entreno con el 50% de los datos\n",
    "X_train_sample = X_train.sample(frac=0.5, random_state=42).copy()\n",
    "y_train_sample = y_train.loc[X_train_sample.index].copy()\n",
    "\n",
    "# Agrupo categorías raras en other\n",
    "cat_cols = X_train_sample.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    # Categorías raras en entrenamiento\n",
    "    counts_train = X_train_sample[col].value_counts()\n",
    "    rare_train = counts_train[counts_train < 5].index\n",
    "    X_train_sample.loc[:, col] = X_train_sample[col].replace(rare_train, \"other\")\n",
    "    \n",
    "    # Categorías válidas para validación\n",
    "    valid_categories = set(X_train_sample[col].unique())\n",
    "    X_val.loc[:, col] = X_val[col].where(X_val[col].isin(valid_categories), \"other\")\n",
    "\n",
    "num_cols = X_train_sample.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Preprocesamiento \n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    (\"num\", \"passthrough\", num_cols)\n",
    "])\n"
   ],
   "id": "e5169184370e6140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelos con diferentes parámetros a probar (Muy muy rápidos)\n",
    "models = {\n",
    "    # Random Forest\n",
    "    \"RF - baja regularización\": RandomForestClassifier(n_estimators=10, max_depth=5, n_jobs=-1, random_state=42),\n",
    "    \"RF - alta regularización\": RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_leaf=5, n_jobs=-1, random_state=42),\n",
    "\n",
    "    # Gradient Boosting\n",
    "    \"GB - baja regularización\": HistGradientBoostingClassifier(max_iter=10, max_depth=4, learning_rate=0.1, random_state=42),\n",
    "    \"GB - alta regularización\": HistGradientBoostingClassifier(max_iter=10, max_depth=3, learning_rate=0.2, random_state=42)\n",
    "}"
   ],
   "id": "4113265fbd7b25f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Entrenamiento y evaluación\n",
    "predictions = {}\n",
    "scores = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n=== Entrenando {name} ===\")\n",
    "    model = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", clf)])\n",
    "    model.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    y_pred_val = model.predict_proba(X_val)[:, 1]\n",
    "    ap_val = average_precision_score(y_val, y_pred_val)\n",
    "    print(f\"Average Precision: {ap_val:.4f}\")\n",
    "    \n",
    "    predictions[name] = y_pred_val\n",
    "    scores[name] = ap_val\n"
   ],
   "id": "42dd291e14ca54b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, y_pred in predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel(\"Falsos Positivos\")\n",
    "plt.ylabel(\"Verdaderos Positivos\")\n",
    "plt.title(\"Curvas ROC - RF y Boosting (ultrarrápido)\")\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Curva Precisión Recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, y_pred in predictions.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred)\n",
    "    ap = average_precision_score(y_val, y_pred)\n",
    "    plt.plot(recall, precision, lw=2, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.title(\"Curvas Precisión–Recall - RF y Boosting (ultrarrápido)\")\n",
    "plt.legend(loc=\"lower left\", fontsize=8)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "df7c4179b9b775ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b994d88d02cfaade"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
